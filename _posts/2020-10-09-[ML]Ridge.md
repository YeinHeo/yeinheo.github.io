# Regularization2

# Ridge Regression

- Least Squares 한계점

    만약 2개의 training data가 있다면 어떻게 될까?

    아마도 least squares를 사용하여 2개의 training data를 지나는 line을 그려 sum of the squared residuals을 0으로 만들 수 있다. 하지만 testing data가 많아질수록 line에 적합하지 않아 sum of the squared residuals은 매우 커지게 되고, line은 high variance를 가진다. 우리는 least squres를 통해 line이 overfit되고 high variance를 가진다는 것을 확인할 수 있다. 이와 같은 한계점으로 Ridge Regression을 통해 적합한 line을 찾는 방법을 고려하게 되었다.

- Least Squares  vs  Ridge Regression

    Least Squares: minimize the sum of the squared residuals

    Ridge Regression: minimize the sum of the squared residuals + λ (the slope)^

- Ridge Regression

![Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled.png](Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled.png)

Ridge Regression은 실제 Y값에다 MSE를 최소화시키는데 베타 제곱을 특정한 값보다 작게 제약하는 것이다.

**λ (slope)^2  ⇒ Ridge Regression Penalty**

slope 제곱은 기존 least squares에 penalty를 더하고, λ는 얼마나 penalty를 줄 것인가를 결정한다.

![Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled%201.png](Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled%201.png)

빨간색: Least Squres
파란색: Ridge Regression

⇒ 위와 같이 Ridge Regression을 사용하면 variance가 적어진다.

**slope**: Least Squares Line보다 Ridge Regression Line이 경사가 더 완만하므로 weight의 변화에 따른 size의 변화가 less sensitive하다.

**λ**: 만약 0이면 least squares와 같아지고, 큰 값을 가지면 slope이 0에 가까워지므로 least squares와 같아진다.

- 큰 λ 값: 간단한 모델, underfit 위험 증가
- 작은 λ 값: 복잡한 모델, overfit 위험 증가

*만약 training set에 only one datapoint가 있다면 Least Squares는 optimal solution을 찾지 못하지만
Ridge Regression은 cross validation 과 Ridge Regression penalty로 solution을 찾을 수 있다.

### Summary

This is done by adding the Ridge Regression Penalty to the thing that must be minimized
**the sum of the squared residuals + λ (slope)^2**

λ is determined using Cross Validation.

When there isn't enough data to find the Least Squares parameter estimates, 
Ridge Regression can still find a solution with Cross Validation and Ridge Regression Penalty.

# Lasso Reression

Lasso Regression은 Ridge Regression인 **the sum of the squared residuals + λ (slope)^2**에서 
**the sum of the squared residuals + λ |slope|**으로 바뀐 것이다.

λ는 0에서 양의 무한대 사이의 값을 가지며 cross validation을 사용하여 결정된다.

- Losso Regression vs Ridge Regression
    - Ridge Regression

        ![Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled%202.png](Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled%202.png)

    - Lasso Regression

        ![Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled%203.png](Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled%203.png)

    - 비슷한점

        λ값이 0이 되면 Least Squares Line과 같아지고, λ값이 점점 증가하고 slope값이 점점 작아지다가 결국 0이 된다.

    - 다른 점

        Ridge Regression은 점차적으로 slope이 0이 되는데 Lasso Regression은 t값을 줄이는 과정에서 중요하지 않은 변수가 t가 0이 되기 전에 바로 0이 된다. 

        또한 Ridge는 모서리(x축이나 y축)에서 정답(MSE랑 만나는 지점-제약조건을 만족하면서 MSE가 최소가 되는 지점)이 나타나지 않는데, Lasso는 절대값이므로 마름모 형태로 나타나게 되는데 이 경우에는 정답이 모서리에서 나타나 관련없는 변수값을 0으로 만든다. 아래는 베타1의 값이 0이 되는 것을 보아 베타1은 중요하지 않다는 것을 알 수 있다.

        ![Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled%204.png](Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled%204.png)

        왼쪽이 Ridge Regression, 오른쪽이 Lasso Regression

- Lasso Regression

![Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled%205.png](Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled%205.png)

LASSO : Least Absolute Shrinkage and Selection Operator

Lasso Regression: Y를 예측하는데 중요한 x를 자동으로 선택하는 모델이다. Ridge Regression의 shrinkage에 selection이 더해진 것이다. 따라서 Lasso는 필요없는 변수들을 줄여 모델의 variance를 줄여준다.

### Summary

Ridge Regression is very similar to Lasso Regression. 

The superficial difference is that Ridge Regression squares the variables and Lasso Regression takes the absolute value. But the big difference is that Lasso Regression can exclude useless variables from equations. 

⇒ simpler and easier to interpret

# Elastic Net Regression

Lasso Regression은 변수들 간의 상관관계가 클 경우에 변수 선택 성능이 저하될 수 있다. 따라서 상관 관계 큰 변수를 동시에 선택하거나 배제하는 특성을 가진 Elastic Net Regression을 알아보자!

- Elastic Net Regression

    ![Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled%206.png](Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled%206.png)

    Elastic Net Regression = Lasso Regression + Ridge Regression 

    good at dealing with situations when there are correlations between parameters

    Elastic net estimator에 대해 다음 부등식이 성립된다.(x(i)와 x(j)는 상관계수)

    ![Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled%207.png](Regularization2%20a0a80103261d4654b8eef47a40481c1b/Untitled%207.png)

    p(ij)가 1이 되면 좌측항은 0보다 작거나 같게 되는데 이때 베타i와 베타j가 같게 된다.

    또한 람다2가 큰 값을 가지게 되어도 베타i와 베타j의 상관관계가 커진다.

    ⇒ Grouping effect라고 해서 correlation을 고려한다.

    ### summary

    Elastic-Net Regression combines the Lasso Regression Penalty with the Ridge Regression Penalty.

    It does a better job dealing with correlated parameters.